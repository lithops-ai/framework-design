{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lithops Design Docs","text":"<p>This is the design documentation for the Lithops Multi-Agent Framework.</p>"},{"location":"communication-mechanism-design/","title":"Lithops - Communication Mechanism Design","text":""},{"location":"communication-mechanism-design/#1-goals-and-background","title":"1. Goals and Background","text":"<ol> <li>In large-scale distributed applications, it is necessary to provide a \"real-time communication + topic/subtopic\" based collaborative model for millions of dispersed Agents (deployed across cloud, edge, IoT devices, etc.).</li> <li>The goal is to enable Agents to interact hierarchically through a \"Discord-like\" user experience, structured around \u201cChannel-Thread-Message,\u201d while supporting access control, group management, attachment handling, message history tracking, and other features.</li> <li>Within cloud-native ecosystems, the solution should leverage Kubernetes CRD + Operator to achieve declarative configuration and fully automated operations. However, it must avoid mapping millions of Agent identities or messages directly onto etcd to minimize overloading the Kubernetes control plane.</li> </ol>"},{"location":"communication-mechanism-design/#2-high-level-architecture","title":"2. High-Level Architecture","text":"<p>To ensure high concurrency and scalability, the system is divided into the following core components:</p> <ol> <li> <p>Kubernetes API Server / etcd</p> <ul> <li>Only stores essential metadata such as \"Channels/Threads\" (as CRDs) and small summaries like \"Groups\" or access control information.</li> <li>Does not store millions of Agent identities or message histories.</li> </ul> </li> <li> <p>Message Queue System (MQ)</p> <ul> <li>Systems like NATS, RabbitMQ, Kafka, etc., are used to handle large-scale, real-time, high-throughput message distribution.</li> <li>Each Channel or Thread corresponds to a Subject/Topic/Queue in the MQ system, enabling isolation similar to \"Discord Channels-Threads.\"</li> </ul> </li> <li> <p>CollaborationPolicyService (External Identity + Access Control Management)</p> <ul> <li>An independent service that manages Agent identities (IDs), Groups, and Role information, as well as access control policies for collaboration scenarios.</li> <li>Operators and the MQ system consult this service to verify Agent permissions for Channels/Threads; simultaneously, Agent clients must authenticate through this service before connecting.</li> </ul> </li> <li> <p>Channel/Thread Operator</p> <ul> <li>Monitors Channel and Thread CRDs defined in Kubernetes, automating the creation and update of their corresponding topics/routing policies in the MQ system.</li> <li>Interacts with CollaborationPolicyService to grant/revoke group access to Channels/Threads without creating a large number of RoleBindings in Kubernetes.</li> </ul> </li> <li> <p>Agents (Millions of Instances)</p> <ul> <li>Deployed on cloud or edge nodes, these connect to the MQ system via SDKs or APIs to publish and subscribe to messages, enabling collaborative interaction.</li> <li>Before connecting, Agents authenticate with the CollaborationPolicyService to receive certificates or tokens, enabling their access to specific Channels/Threads.</li> </ul> </li> </ol>"},{"location":"communication-mechanism-design/#3-key-requirements-and-challenges","title":"3. Key Requirements and Challenges","text":"<ol> <li> <p>Massive Scale &amp; Multi-Agent Concurrency</p> <ul> <li>Millions of Agents need to communicate simultaneously, publishing/subscribing to messages with high-speed. Traditional monolithic systems are inadequate for this scale.</li> </ul> </li> <li> <p>Hierarchical Access Control</p> <ul> <li>Different Agents possess different roles (e.g., Owner, Moderator, Member), but each Agent's role cannot be mapped into Kubernetes RoleBindings. Access control must be externalized.</li> </ul> </li> <li> <p>Real-Time Messaging and History</p> <ul> <li>Similar to a \u201cDiscord-like\u201d system, the platform must deliver low-latency message distribution while supporting searchable and replayable message history in Channels/Threads.</li> </ul> </li> <li> <p>Cloud-Native Scalability</p> <ul> <li>The system should seamlessly scale within Kubernetes, leveraging Operators for fully automated operations, minimizing the need for manual management of message middleware configurations.</li> </ul> </li> </ol>"},{"location":"communication-mechanism-design/#4-logical-layer-design","title":"4. Logical Layer Design","text":""},{"location":"communication-mechanism-design/#41-core-concepts","title":"4.1 Core Concepts","text":"<ol> <li> <p>Channel</p> <ul> <li>Represents a discussion space for a group, topic, or major task, which can be public or private.</li> <li>Configurable attributes include visibility (public/private), read/write policies, persistence options, etc.</li> <li>Most Agents do not directly interact with Kubernetes; instead, they depend on the CollaborationPolicyService to determine whether they can \u201cjoin\u201d a given Channel.</li> </ul> </li> <li> <p>Thread</p> <ul> <li>A sub-discussion space within a Channel, inheriting or defining separate access permissions.</li> <li>Helps segregate and focus discussions to avoid overwhelming the main Channel.</li> </ul> </li> <li> <p>Message</p> <ul> <li>The core communication unit containing text, attachments, mentions, etc.</li> <li>Messaging payloads are not recommended for storage in etcd. Instead, MQ or external databases should handle them for real-time processing and historical storage.</li> </ul> </li> <li> <p>Mentions</p> <ul> <li>For example, \u201c@AgentX\u201d or \u201c@Group,\u201d enabling priority notifications or targeted pushing via MQ.</li> <li>CollaborationPolicyService resolves aliases into actual Agent IDs.</li> </ul> </li> <li> <p>Roles</p> <ul> <li>Owner: Manages Channel configurations, adds/removes groups.</li> <li>Moderator: Can administratively block specific groups or Agents (maintained through the CollaborationPolicyService).</li> <li>Member: Regular participants sending/receiving messages in Channels.</li> <li>Roles are handled exclusively in CollaborationPolicyService\u2014not within Kubernetes itself.</li> </ul> </li> </ol>"},{"location":"communication-mechanism-design/#5-implementation-details","title":"5. Implementation Details","text":""},{"location":"communication-mechanism-design/#51-simplified-crd-design","title":"5.1 Simplified CRD Design","text":""},{"location":"communication-mechanism-design/#511-channel-crd","title":"5.1.1 Channel CRD","text":"<p>Instead of maintaining RoleBinding for every individual Agent, it only contains minimal group-based metadata.</p> <pre><code>apiVersion: agents.platform.io/v1\nkind: Channel\nmetadata:\n  name: sensor-anomaly-channel\nspec:\n  visibility: \"private\"  # Other values: \"public\"\n  authorizedGroups:\n    - \"EdgeSensorsGroup\"\n    - \"MLWorkersGroup\"\n  enablePersistence: true    # Enables historical storage in MQ\n  retentionPolicy: \"7d\"      # Operator parses this field\nstatus:\n  natsSubject: \"channels.sensors.anomaly\"\n  state: \"Active\"\n</code></pre>"},{"location":"communication-mechanism-design/#512-thread-crd","title":"5.1.2 Thread CRD","text":"<pre><code>apiVersion: agents.platform.io/v1\nkind: Thread\nmetadata:\n  name: anomaly-subanalysis\nspec:\n  channel: \"sensor-anomaly-channel\"\n  participantsGroups:\n    - \"AnomalyFocusedGroup\"  # Can inherit from parent Channel or define separately\n  topic: \"Sensor anomalies sub-thread\"\nstatus:\n  natsSubject: \"channels.sensors.anomaly.thread-xyz\"\n  state: \"Ongoing\"\n</code></pre>"},{"location":"communication-mechanism-design/#52-operators","title":"5.2 Operators","text":""},{"location":"communication-mechanism-design/#521-channel-operator","title":"5.2.1 Channel Operator","text":"<ol> <li>Monitoring Channel CRDs for Create/Update/Delete operations.</li> <li> <p>On detecting a new Channel:</p> <ul> <li>Use MQ APIs (e.g., NATS Admin) to create a corresponding topic (Subject/Topic/Queue).</li> <li>Update the authorization policies in CollaborationPolicyService for designated <code>authorizedGroups</code>.</li> <li>Adhere to <code>enablePersistence</code> and <code>retentionPolicy</code> to configure MQ persistence policies.</li> </ul> </li> <li> <p>On Channel Updates:</p> <ul> <li>Adjust CollaborationPolicyService permissions if <code>authorizedGroups</code> are modified.</li> <li>Update MQ persistence policies if <code>enablePersistence</code> or <code>retentionPolicy</code> changes.</li> </ul> </li> <li> <p>On Channel Deletion:</p> <ul> <li>Mark the MQ topic for archival or cleanup. Physically delete the topic after a delay (optional).</li> <li>Revoke associated authorizations in CollaborationPolicyService.</li> </ul> </li> </ol>"},{"location":"communication-mechanism-design/#522-thread-operator","title":"5.2.2 Thread Operator","text":"<ol> <li>Monitoring Thread CRDs for CRUD events.</li> <li> <p>Logic mirrors that of the Channel Operator:</p> <ul> <li>Create sub-topics in MQ for Threads.</li> <li>Update permitted <code>participantsGroups</code> or inherit them from the parent Channel.</li> </ul> </li> <li> <p>When Threads are archived or deleted:</p> <ul> <li>Remove related sub-topics from the MQ and adjust retention policies accordingly.</li> </ul> </li> </ol>"},{"location":"communication-mechanism-design/#53-collaborationpolicyservice","title":"5.3 CollaborationPolicyService","text":"<ul> <li>An independent external or microservice-based system that leverages specialized databases (Postgres, Redis, NoSQL, etc.) to store identity, group, and role information for millions of Agents.</li> <li>Exposes APIs like <code>CheckPermission(AgentID, Subject)</code> and <code>UpdateChannelAuth(ChannelID, GroupList)</code> for use by Operators and MQ systems alike.</li> <li>Before an Agent attempts to join any Channel, it undergoes authentication and, upon success, receives an MQ access token allowing it to communicate securely.</li> </ul>"},{"location":"communication-mechanism-design/#54-mq-selection-and-configuration","title":"5.4 MQ Selection and Configuration","text":"<p>Let\u2019s take NATS as an example (alternatives include RabbitMQ or Kafka):</p> <ol> <li> <p>NATS on Kubernetes:</p> <ul> <li>Deploy NATS using a Kubernetes NATS Operator for automated scaling and failover.</li> </ul> </li> <li> <p>Subject Naming Convention:</p> <ul> <li>Tie Subjects to CRDs, e.g., <code>channels.&lt;channelName&gt;.threads.&lt;threadName&gt;</code>.</li> </ul> </li> <li> <p>JetStream Persistence:</p> <ul> <li>With persistence enabled, set up JetStream to manage histories via automatic Message Replay mechanisms.</li> </ul> </li> <li> <p>ACL and Security:</p> <ul> <li>Utilize NATS multi-tenant or Account-based ACL management. Allow group-specific tokens via CollaborationPolicyService, enabling secure logins.</li> </ul> </li> </ol>"},{"location":"communication-mechanism-design/#6-typical-messaging-workflow-example","title":"6. Typical Messaging Workflow Example","text":"<ol> <li> <p>Channel Creation:</p> <ul> <li>An Administrator submits a <code>Channel</code> CRD (\u201csensor-anomaly-channel\u201d), specifying <code>authorizedGroups = [\u201cEdgeSensorsGroup\u201d, \u201cMLWorkersGroup\u201d]</code>.</li> <li>The Channel Operator:<ul> <li>Creates a matching NATS Subject, \u201cchannels.sensors.anomaly.\u201d</li> <li>Updates CollaborationPolicyService to grant <code>EdgeSensorsGroup</code> and <code>MLWorkersGroup</code> access.</li> </ul> </li> </ul> </li> <li> <p>Agent Joins &amp; Sends Messages:</p> <ul> <li>AgentX (part of EdgeSensorsGroup) authenticates with CollaborationPolicyService and receives a token for \u201cchannels.sensors.anomaly.\u201d</li> <li>AgentX subscribes to the topic and pushes anomaly detection messages.</li> </ul> </li> <li> <p>Thread Creation:</p> <ul> <li>To deep-dive into details, a Thread CRD is created for a sub-topic (\u201canomaly-subanalysis\u201d).</li> <li>Thread Operator provisions the NATS sub-channel and updates permissions.</li> </ul> </li> <li> <p>Historical Replay:</p> <ul> <li>If JetStream persistence is enabled, Agents can replay historical messages, or archival tasks can operate seamlessly.</li> </ul> </li> </ol>"},{"location":"communication-mechanism-design/#7-advantages-and-value","title":"7. Advantages and Value","text":"<ol> <li> <p>Scalability:</p> <ul> <li>Offloads identity and message management to external systems, keeping etcd and Kubernetes Control Plane lightweight.</li> </ul> </li> <li> <p>Cloud-Native Integration:</p> <ul> <li>Stays declarative, leveraging Kubernetes Operators to seamlessly integrate with CRDs and external services.</li> </ul> </li> <li> <p>Flexibility:</p> <ul> <li>MQ (NATS, Kafka, etc.) and CollaborationPolicyService implementations are modular\u2014easy to swap or customize.</li> </ul> </li> <li> <p>Discord-like Experience:</p> <ul> <li>Fully supports Channels, Threads, Mentions, message searchability, and hierarchical permissions.</li> </ul> </li> <li> <p>Future Expansion:</p> <ul> <li>Extensible for advanced integrations, such as embedding LLMService Operators in the future.</li> </ul> </li> </ol>"},{"location":"communication-mechanism-design/#8-conclusion","title":"8. Conclusion","text":"<p>This design leverages Kubernetes CRDs and Operators to declaratively automate \u201cChannel/Thread\u201d management while offloading large-scale identity/auth and message processing to external systems (CollaborationPolicyService + MQ). This approach reduces Kubernetes control-plane dependency and delivers high scalability, paving the way for a \u201cDiscord-like\u201d system for multi-Agent communication. Future extensions (e.g., AI/LLM-based collaboration) can easily integrate into this architecture, maintaining a unified cloud-native operations model.</p>"},{"location":"entity-component-system/","title":"Overview","text":"<p>Lithops takes the entity-component-system (ECS) architecture from game development and scales it up to support millions of entities in a decentralized fashion.</p>"},{"location":"entity-component-system/implementation-architecture/","title":"Implementation Architecture","text":"<p>Note</p> <p>This is just a tentative design.</p> <p>This document discusses a number of approaches for implementing large-scale, distributed and decentralized entity-component systems.</p> <p>Currently, we take the third approach, i.e., runtime component injection.</p>"},{"location":"entity-component-system/implementation-architecture/#architecture-choices","title":"Architecture Choices","text":"<p>There are several schemes to implement the Entity-Component-System (ECS) architecture.</p>"},{"location":"entity-component-system/implementation-architecture/#one-container-per-actor","title":"One Container Per Actor","text":"<p>This is the easiest scheme to think of. In this scheme, each actor is associated with a single container, and all components in that actor are isolated in that container.</p> <p>Pros:</p> <ul> <li>Each actor with its components is isolated, and failure within one actor does not affect the others.</li> <li>There is minimal communication cost between multiple components on a single actor.</li> </ul> <p>Cons:</p> <ul> <li>Even though the number of components is limited, the number of possible combinations, and hence the number of actor types, can be potentially large. If we were to create an image for each actor with a different set of components, the number of docker images would blow up.</li> <li>Does not allow adding or removing components at runtime.</li> </ul>"},{"location":"entity-component-system/implementation-architecture/#one-service-per-component","title":"One Service Per Component","text":"<p>In this scheme, each type of component is implemented as a service which automatically load-balances across multiple nodes as the number of actors using this component grows. All actors using the same type of component connects to the same service endpoint.</p>"},{"location":"entity-component-system/implementation-architecture/#general-architecture","title":"General Architecture","text":"<p>Physically, each type of component is exposed as a service which may be deployed over one or multiple nodes. The idea is, component services are guaranteed to be always available, and scales automatically with the number of actors using them.</p> <p>For example, consider a multi-agent system:</p> <pre><code>graph TD\n    A1(Actor 1) --&gt; C11(Component 1)\n    A1 --&gt; C12(Component 2)\n    A2(Actor 2) --&gt; C21(Component 1)</code></pre> <p>The implementation might look like:</p> <pre><code>graph TD\n    A1(Actor 1) --&gt; C1\n    A2(Actor 2) --&gt; C1\n    A1 --&gt; C2\n    C1(Component 1 Service)\n    C2(Component 2 Service)\n    C1 --&gt; N1(Node 1)\n    C1 --&gt; N2(Node 2)\n    C2 --&gt; N2</code></pre>"},{"location":"entity-component-system/implementation-architecture/#component-service-implementation","title":"Component Service Implementation","text":"<p>To ensure isolation across different component instances, each instance of a component is encapsulated in its own process.</p> <p>Pros:</p> <ul> <li>Scales to large number of actors with different combinations of components.</li> <li>Allows dynamically adding and removing components   after an actor is spawned (like in game engines).</li> <li>Scales well in situations where there is a single actor so large it cannot fit on one machine, as components in an actor are distributed across multiple services and nodes in nature.</li> </ul> <p>Cons:</p> <ul> <li>It can be hard to ensure isolation between   different instances of the same component type   attached to different actors,   especially when those instances are deployed on the same node.</li> <li>Each inter-component event requires a round-trip over the cluster network,   even for the components on the same actor.</li> </ul>"},{"location":"entity-component-system/implementation-architecture/#runtime-component-injection","title":"Runtime Component Injection","text":"<p>This scheme is similar to \"One Container Per Actor\", in that each actor gets its own container. The difference is that instead of creating an image for each different combination of components, there is only one image providing the basic functionalities like event passing. When an actor is spawn, a container is created, then the components are injected into the container.</p> <p>The critical design choice with this architecture is how to package the runtime code for each component. Apparently, we want something more lightweight than a container since the components are already running in a container. May look into existing sandboxing solutions like snap and AppImage.</p> <p>In general, the means of component packaging/virtualization should:</p> <ol> <li>Be lightweight;</li> <li>Ensure isolation between different components on the same actor.</li> </ol> <p>Pros:</p> <ul> <li>Ensures isolation between different actors.</li> <li>Keeps the number of packages at a minimum even when there are a large number of actors with different combinations of components.</li> <li>Allows dynamically adding and removing components after an actor is spawned (like in game engines).</li> </ul> <p>Cons:</p> <ul> <li>The process of component injection can introduce unexpected bugs. For example, if two components use the same temporary directory in the host container, they may interfere with each other and cause unwanted behavior even if they have no bugs on their own.</li> </ul>"},{"location":"entity-component-system/implementation-architecture/#runtime-component-injection_1","title":"Runtime Component Injection","text":"<p>Currently, we employ the runtime component injection approach for implementing the ECS architecture.</p> <p>The design ideas are:</p> <ul> <li>Components may be buggy so there needs to be some sandboxing for each component.</li> <li>Component sandboxing should incur minimal overhead since we're already inside a container.</li> <li>Components should be self-contained to avoid dependency and environment configuration issues.</li> </ul> <p>Specifically:</p> <ul> <li>Use AppImage to package each component into a self-contained binary.</li> <li>Use BubbleWrap to sandbox each component AppImage and provide a safe, ephemeral mount point for it to write to, similar to a Docker volume (expect that it does not persist). Restrict AppImages from accessing the host filesystem to avoid multiple components sharing the same file system and interfering with each other.</li> </ul>"},{"location":"entity-component-system/logical-abstraction/","title":"Logical Abstraction","text":"<p>Lithops employs an entity-component-system (ECS) style architecture.</p> <p>In this architecture, everything in a multi-agent system is abstracted as an actor. This includes agents capable of critical thinking, frontend that interacts with the user, and even API services.</p> <p>An actor on its own does not do anything useful; its behavior is defined by the components attached to it.</p> <p>A component is a reusable piece of logic that adds a certain behavior to an actor.</p>"},{"location":"entity-component-system/logical-abstraction/#communication","title":"Communication","text":"<p>Actors and components communicate through events. Components may expose event dispatchers for other components from either the same or another actor to bind to. Of course, API calls are also supported; a component can call an exposed API from other components to process data or to invoke certain behavior.</p>"},{"location":"entity-component-system/logical-abstraction/#actor-discovery","title":"Actor Discovery","text":"<p>Actor discovery refers to how an actor finds other actors it needs to reference. For finding \"global actors\" such as API services and user interface component, we provide a tagging mechanism similar to those find in Unity and Unreal, where:</p> <ul> <li>Each actor can have a bunch of tags.</li> <li>There is a system-wide service for finding actor/actors with a specific tag.</li> </ul>"},{"location":"entity-component-system/logical-abstraction/#actor-virtualization","title":"Actor Virtualization","text":"<p>Actors are virtualized and automatically restarted/repaired on any non-intentional failures such as network disconnection or machine failures. From the user's perspective, actors are always alive and available unless intentionally destroyed.</p>"},{"location":"entity-component-system/logical-abstraction/#examples","title":"Examples","text":"<p>For a better understanding of the ECS architecture, let's look at some examples. Consider a simple multi-agent system that develops software from specification provided by the user. This system consists of the following actors:</p> <ul> <li>A frontend for interacting with the user.   Components:<ul> <li>A \"UserSoftwareSpecInput\" component that exposes an event dispatcher   which fires whenever the user submits a new software development request.</li> <li>A \"ReportOnDevComplete\" component   that reports to the user when a   software development goal completion event   (exposed by the LLMGoalFinisher component)   is fired.</li> </ul> </li> <li>A software developer agent for building software.   Components:<ul> <li>An \"LLMGoalFinisher\" component representing the agent's brain.   This component takes a goal, produces a plan and executes it.   During goal execution, it may call other tools.   This component exposes an event dispatcher to be fired   when the development of a software is complete.</li> <li>A \"ForwardGoal\" component that sets the goal of LLMGoalFinisher component   whenever the user inputs a software development request.</li> </ul> </li> <li>A compiler service that takes the source code and outputs the compiled binary.   Components:<ul> <li>A \"CompilerService\" component that exposes a software compilation API.</li> </ul> </li> </ul>"},{"location":"general-design/","title":"Lithops General Design","text":"<p>This document outlines the general design of Lithops.</p>"},{"location":"general-design/design-philosophy/","title":"Design Philosophy","text":"<p>Lithops is designed around a set of ideas and goals: robustness &amp; decentralized control, separation of logical/physical construct and flexibility.</p>"},{"location":"general-design/design-philosophy/#robustness-decentralization","title":"Robustness &amp; Decentralization","text":"<p>Lithops deploys a large number of self-autonomous agents over a cluster. There is no centralized control and the failure of one agent has minimal impact over the others.</p>"},{"location":"general-design/design-philosophy/#separation-of-logicalphysical-construct","title":"Separation of Logical/Physical Construct","text":"<p>In Lithops, the logical abstraction of a multi-agent system and how it functions physically are decoupled.</p>"},{"location":"general-design/design-philosophy/#flexibility","title":"Flexibility","text":"<p>At its core, Lithops tries to introduce minimal inductive bias over how agents should be designed and how they behave.</p> <p>Rather than a monolithic framework, Lithops is a set of building blocks that can be composed to create a wide range of multi-agent systems. Some of these building blocks may have assumptions of a certain aspect of agents, and they may have dependency on other building blocks. These building blocks may be considered subsystems or micro-frameworks; they interact with each other according to protocols, a concept similar to trait in Rust.</p>"}]}